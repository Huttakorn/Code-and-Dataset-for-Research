{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aGcpHMU2SYY"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "pip install gensim pandas pyLDAvis\n",
        "\n",
        "import pandas as pd  # Import pandas for data manipulation\n",
        "from gensim import corpora, models  # Import gensim for topic modeling\n",
        "from gensim.models.coherencemodel import CoherenceModel  # Import CoherenceModel for evaluating topic coherence\n",
        "import nltk  # Import nltk for natural language processing\n",
        "from nltk.stem import WordNetLemmatizer  # Import lemmatizer from nltk\n",
        "from nltk.corpus import wordnet, stopwords  # Import wordnet and stopwords from nltk\n",
        "import pyLDAvis  # Import pyLDAvis for visualizing LDA models\n",
        "import pyLDAvis.gensim_models as gensimvis  # Import gensimvis for integrating LDA visualization with pyLDAvis\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Initialize lemmatizer and stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "custom_stop_words = { }\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.update(custom_stop_words)\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "def preprocess(text):\n",
        "    # Tokenize, lemmatize, and remove stop words\n",
        "    return [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in text.lower().split() if w not in stop_words]\n",
        "\n",
        "# Load the datasets\n",
        "df1 = pd.read_csv('dataset_path_for_topic_modeling')\n",
        "df2 = pd.read_csv('dataset_path_for_topic_weight')\n",
        "\n",
        "# Preprocessing: Apply tokenization, lemmatization, and stopword removal\n",
        "df1['processed'] = df1['line'].apply(preprocess)\n",
        "df2['processed'] = df2['line'].apply(preprocess)\n",
        "\n",
        "# Create a dictionary and corpus for topic modeling\n",
        "dictionary = corpora.Dictionary(df1['processed'])\n",
        "\n",
        "# Filter out words that appear too frequently or too rarely\n",
        "dictionary.filter_extremes(no_below=32)\n",
        "\n",
        "# Create a bag-of-words corpus\n",
        "corpus = [dictionary.doc2bow(text) for text in df1['processed']]\n",
        "\n",
        "# Optimizing hyperparameters\n",
        "num_topics = 19  # Adjust as needed\n",
        "passes = 20\n",
        "iterations = 560\n",
        "alpha = 'auto'  # or you can use a specific value like 'symmetric', 'asymmetric', or a fixed number\n",
        "eta = 'auto'  # or a specific value\n",
        "random_seed = 100  # Random seed for reproducibility\n",
        "\n",
        "# Train the LDA model with optimized hyperparameters\n",
        "lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary,\n",
        "                            passes=passes, iterations=iterations, alpha=alpha, eta=eta, random_state=random_seed)\n",
        "\n",
        "# Evaluate topic coherence\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=df1['processed'], dictionary=dictionary, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print(f'Topic Coherence: {coherence_lda}')\n",
        "\n",
        "# Display topics\n",
        "for idx, topic in lda_model.print_topics(-1, num_words=10):\n",
        "    print(f'Topic: {idx} \\nWords: {topic}\\n')\n",
        "\n",
        "# Analyze topic weight in the second dataset\n",
        "corpus2 = [dictionary.doc2bow(text) for text in df2['processed']]\n",
        "topic_weights = [lda_model[doc] for doc in corpus2]\n",
        "\n",
        "# Example: Print topic weights for the first document in the second dataset\n",
        "print(f'Topic weights for the first document: {topic_weights[0]}')\n",
        "\n",
        "# Create a DataFrame from the topic weights\n",
        "topic_weights_df = pd.DataFrame([\n",
        "    {**{'Filename': filename, 'WordCount': wordcount}, **{f'Topic {topic}': weight for topic, weight in doc}}\n",
        "    for filename, wordcount, doc in zip(df2['filename'], df2['wordcount'], topic_weights)\n",
        "])\n",
        "\n",
        "# Fill NaN values with 0 (indicating no weight/absence in that document)\n",
        "topic_weights_df = topic_weights_df.fillna(0)\n",
        "\n",
        "# Optional: Renaming columns to a more readable format\n",
        "topic_weights_df.columns = ['Filename', 'WordCount'] + [f'Topic {col}' if col.startswith('Topic') else col for col in topic_weights_df.columns[2:]]\n",
        "\n",
        "# Display the DataFrame\n",
        "print(topic_weights_df.head())\n",
        "\n",
        "# Save the DataFrame to CSV if needed\n",
        "topic_weights_df.to_csv('file_path', index=False)\n",
        "\n",
        "# Analyze topic weight in the first dataset\n",
        "topic_weights_sample = [lda_model[doc] for doc in corpus]\n",
        "\n",
        "# Example: Print topic weights for the first document in the first dataset\n",
        "print(f'Topic weights for the first document: {topic_weights_sample[0]}')\n",
        "\n",
        "# Create a DataFrame from the topic weights\n",
        "topic_weights_df2 = pd.DataFrame([\n",
        "    {**{'Filename': line}, **{f'Topic {topic}': weight for topic, weight in doc}}\n",
        "    for line, doc in zip(df1['line'], topic_weights_sample)\n",
        "])\n",
        "\n",
        "# Fill NaN values with 0 (indicating no weight/absence in that document)\n",
        "topic_weights_df2 = topic_weights_df2.fillna(0)\n",
        "\n",
        "# Optional: Renaming columns to a more readable format\n",
        "# topic_weights_df2.columns = ['Filename'] + [f'Topic {col}' if col.startswith('Topic') else col for col in topic_weights_df2.columns[:]]\n",
        "\n",
        "# Display the DataFrame\n",
        "print(topic_weights_df2.head())\n",
        "\n",
        "# Save the DataFrame to CSV if needed\n",
        "topic_weights_df2.to_csv('file_path', index=False)\n",
        "\n",
        "# Prepare the visualization\n",
        "pyLDAvis.enable_notebook()\n",
        "lda_vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
        "\n",
        "# Save the visualization as an HTML file\n",
        "pyLDAvis.save_html(lda_vis, 'file_path')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}