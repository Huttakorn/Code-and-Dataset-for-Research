{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#To use this script, provide the path to a zip file containing PDF files and an output folder path. The script will extract text from the PDFs, clean and process the text, and save the aggregated data for topic modeling in a CSV file.\n",
        "\n",
        "# Install necessary libraries\n",
        "pip install PyMuPDF\n",
        "\n",
        "import fitz  # PyMuPDF for working with PDF files\n",
        "import zipfile  # Import the zipfile module to work with zip files\n",
        "import os  # Import the os module for file and directory operations\n",
        "import nltk  # Import the nltk module for natural language processing\n",
        "import re  # Import the regular expressions library\n",
        "from nltk.corpus import stopwords  # Import stopwords from nltk corpus\n",
        "from nltk.tokenize import word_tokenize  # Import word_tokenize from nltk for tokenizing words\n",
        "import pandas as pd  # Import pandas for data manipulation\n",
        "\n",
        "# Ensure necessary NLTK data is downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def extract_3fullstopbase(zip_file_path, output_folder):\n",
        "    # Step 1: Extract all PDF files from the zip file\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(output_folder)\n",
        "\n",
        "    # Define custom stop words\n",
        "    custom_stop_words = { }\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stop_words.update(custom_stop_words)\n",
        "\n",
        "    # Step 2: Extract text from each PDF file\n",
        "    for root, dirs, files in os.walk(output_folder):\n",
        "        for file in files:\n",
        "            if file.endswith(\".pdf\") and not file.startswith('.') and '__MACOSX' not in root:\n",
        "                pdf_path = os.path.join(root, file)\n",
        "\n",
        "                try:\n",
        "                    doc = fitz.open(pdf_path)\n",
        "                    text = \"\"\n",
        "                    for page in doc:\n",
        "                        text += page.get_text()\n",
        "                    doc.close()\n",
        "\n",
        "                    # Step 3: Pre-process the text\n",
        "                    # Various replacements to clean up the text\n",
        "                    replacements = { }\n",
        "                    for old, new in replacements.items():\n",
        "                        text = text.replace(old, new)\n",
        "\n",
        "                    text = text.lower()\n",
        "\n",
        "                    # Keep only English alphabet and full stop\n",
        "                    text = re.sub(r'[^a-zA-Z. ]', '', text)\n",
        "\n",
        "                    # Tokenize the text\n",
        "                    words = word_tokenize(text)\n",
        "\n",
        "                    # Remove stop words and numeric tokens\n",
        "                    filtered_text = [word for word in words if word.lower() not in stop_words and not word.isnumeric()]\n",
        "\n",
        "                    # Reconstruct the text\n",
        "                    cleaned_text = ' '.join(filtered_text)\n",
        "\n",
        "                    # Post-processing to clean and format the text\n",
        "                    post_replacements = { }\n",
        "                    for old, new in post_replacements.items():\n",
        "                        cleaned_text = cleaned_text.replace(old, new)\n",
        "\n",
        "                    cleaned_text = cleaned_text.replace('.', '')\n",
        "\n",
        "                    # Eliminate lines with fewer than 20 alphabetic characters\n",
        "                    cleaned_lines = [line for line in cleaned_text.split('\\n') if sum(c.isalpha() for c in line) >= 20]\n",
        "                    cleaned_text = '\\n'.join(cleaned_lines)\n",
        "\n",
        "                    # Save the cleaned text to a file\n",
        "                    text_file_name = os.path.splitext(file)[0] + '.txt'\n",
        "                    text_file_path = os.path.join(root, text_file_name)\n",
        "\n",
        "                    with open(text_file_path, 'w', encoding='utf-8') as text_file:\n",
        "                        text_file.write(cleaned_text)\n",
        "\n",
        "                    print(f\"Converted and modified '{file}' to '{text_file_name}'\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to convert '{file}': {e}\")\n",
        "\n",
        "    # Step 4: Aggregate cleaned text for topic modeling\n",
        "    # Path to the folder containing the text files\n",
        "    text_folder_path = output_folder\n",
        "\n",
        "    # Initialize a list to store the data\n",
        "    data = []\n",
        "\n",
        "    # List all the text files in the folder\n",
        "    text_files = [f for f in os.listdir(text_folder_path) if f.endswith('.txt')]\n",
        "\n",
        "    for text_file in text_files:\n",
        "        # Construct the full file path\n",
        "        text_file_path = os.path.join(text_folder_path, text_file)\n",
        "\n",
        "        # Initialize a list to store lines for the current file\n",
        "        lines = []\n",
        "\n",
        "        # Open and read the text file\n",
        "        with open(text_file_path, 'r', encoding='utf-8') as file:\n",
        "            for line in file:\n",
        "                # Strip leading and trailing whitespace from the line\n",
        "                clean_line = line.strip()\n",
        "                # Append the line to the lines list\n",
        "                lines.append(clean_line)\n",
        "\n",
        "        # Aggregate every three lines into one row\n",
        "        for i in range(0, len(lines), 3):\n",
        "            # Join three lines, if less than three remain, join all remaining\n",
        "            aggregated_line = ' '.join(lines[i:i+3])\n",
        "            # Count the words in the aggregated line\n",
        "            wordcount = len(aggregated_line.split())\n",
        "            # Append the aggregated line, filename, and word count to the data list\n",
        "            data.append({'line': aggregated_line, 'filename': text_file, 'wordcount': wordcount})\n",
        "\n",
        "    # Step 5: Create a DataFrame from the data\n",
        "    dft = pd.DataFrame(data)\n",
        "\n",
        "    # Show the DataFrame\n",
        "    print(dft)\n",
        "\n",
        "    # Step 6: Save the DataFrame to a CSV file\n",
        "    dft.to_csv('file_path', index=False)\n",
        "\n",
        "    print(\"File saved successfully.\")\n"
      ],
      "metadata": {
        "id": "xy53bQv0mzZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To use this script, provide the path to a zip file containing PDF files and an output folder path. The script will extract text from the PDFs, clean and process the text, and save the aggregated data for topic modeling in a CSV file.\n",
        "\n",
        "# Install necessary libraries\n",
        "pip install PyMuPDF\n",
        "\n",
        "import fitz  # PyMuPDF for working with PDF files\n",
        "import zipfile  # Import the zipfile module to work with zip files\n",
        "import os  # Import the os module for file and directory operations\n",
        "import nltk  # Import the nltk module for natural language processing\n",
        "import re  # Import the regular expressions library\n",
        "from nltk.corpus import stopwords  # Import stopwords from nltk corpus\n",
        "from nltk.tokenize import word_tokenize  # Import word_tokenize from nltk for tokenizing words\n",
        "import pandas as pd  # Import pandas for data manipulation\n",
        "\n",
        "# Ensure necessary NLTK data is downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def extract_filebase(zip_file_path, output_folder):\n",
        "    # Step 1: Create output folder if it doesn't exist\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # Step 2: Extract all PDF files from the zip file into the output folder\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(output_folder)\n",
        "\n",
        "    # Define custom stop words\n",
        "    custom_stop_words = { }\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stop_words.update(custom_stop_words)\n",
        "\n",
        "    # Step 3: Extract text from each PDF file\n",
        "    for root, dirs, files in os.walk(output_folder):\n",
        "        for file in files:\n",
        "            if file.endswith(\".pdf\") and not file.startswith('.') and '__MACOSX' not in root:\n",
        "                pdf_path = os.path.join(root, file)\n",
        "\n",
        "                try:\n",
        "                    doc = fitz.open(pdf_path)\n",
        "                    text = \"\"\n",
        "                    for page in doc:\n",
        "                        text += page.get_text()\n",
        "                    doc.close()\n",
        "\n",
        "                    # Step 4: Pre-process the text\n",
        "                    # Various replacements to clean up the text\n",
        "                    replacements = { }\n",
        "\n",
        "                    for old, new in replacements.items():\n",
        "                        text = text.replace(old, new)\n",
        "\n",
        "                    text = text.lower()\n",
        "\n",
        "                    # Keep only English alphabet and full stop\n",
        "                    text = re.sub(r'[^a-zA-Z. ]', '', text)\n",
        "\n",
        "                    # Tokenize the text\n",
        "                    words = word_tokenize(text)\n",
        "\n",
        "                    # Remove stop words and numeric tokens\n",
        "                    filtered_text = [word for word in words if word.lower() not in stop_words and not word.isnumeric()]\n",
        "\n",
        "                    # Reconstruct the text\n",
        "                    cleaned_text = ' '.join(filtered_text)\n",
        "\n",
        "                    # Eliminate lines with fewer than 20 alphabetic characters\n",
        "                    cleaned_lines = [line for line in cleaned_text.split('\\n') if sum(c.isalpha() for c in line) >= 20]\n",
        "                    cleaned_text = '\\n'.join(cleaned_lines)\n",
        "\n",
        "                    # Step 5: Post-process the text\n",
        "                    post_replacements = { }\n",
        "                    for old, new in post_replacements.items():\n",
        "                        cleaned_text = cleaned_text.replace(old, new)\n",
        "\n",
        "                    cleaned_text = cleaned_text.replace('.', '')\n",
        "\n",
        "                    # Save the cleaned text to a file\n",
        "                    text_file_name = os.path.splitext(file)[0] + '.txt'\n",
        "                    text_file_path = os.path.join(root, text_file_name)\n",
        "\n",
        "                    with open(text_file_path, 'w', encoding='utf-8') as text_file:\n",
        "                        text_file.write(cleaned_text)\n",
        "\n",
        "                    print(f\"Converted and modified '{file}' to '{text_file_name}'\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to convert '{file}': {e}\")\n",
        "\n",
        "    # Step 6: Aggregate cleaned text for topic modeling\n",
        "    # Path to the folder containing the text files\n",
        "    text_folder_path = output_folder\n",
        "\n",
        "    # Initialize a list to store the data\n",
        "    data = []\n",
        "\n",
        "    # List all the text files in the folder\n",
        "    text_files = [f for f in os.listdir(text_folder_path) if f.endswith('.txt')]\n",
        "\n",
        "    for text_file in text_files:\n",
        "        # Construct the full file path\n",
        "        text_file_path = os.path.join(text_folder_path, text_file)\n",
        "\n",
        "        # Initialize a list to store lines for the current file\n",
        "        lines = []\n",
        "\n",
        "        # Open and read the text file\n",
        "        with open(text_file_path, 'r', encoding='utf-8') as file:\n",
        "            for line in file:\n",
        "                # Strip leading and trailing whitespace from the line\n",
        "                clean_line = line.strip()\n",
        "                # Append the line to the lines list\n",
        "                lines.append(clean_line)\n",
        "\n",
        "        # Step 7: Process each line in the text file\n",
        "        for i in range(len(lines)):\n",
        "            # Get the current line\n",
        "            current_line = lines[i]\n",
        "            # Count the words in the current line\n",
        "            wordcount = len(current_line.split())\n",
        "            # Append the current line, filename, and word count to the data list\n",
        "            data.append({'line': current_line, 'filename': text_file, 'wordcount': wordcount})\n",
        "\n",
        "    # Step 8: Create a DataFrame from the data\n",
        "    dfd = pd.DataFrame(data)\n",
        "\n",
        "    # Show the DataFrame\n",
        "    print(dfd)\n",
        "\n",
        "    # Step 9: Save the DataFrame to a CSV file\n",
        "    dfd.to_csv('file_path', index=False)\n",
        "\n",
        "    print(\"File saved successfully.\")"
      ],
      "metadata": {
        "id": "WNGehXjdPxtZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}